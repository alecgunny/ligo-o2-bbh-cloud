{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBH End-to-End Inference Analysis Plots\n",
    "\n",
    "The key questions I'm trying to address with the plots below are:\n",
    "- How *quickly* can I process a given volume of data at a given scale of compute?\n",
    "- How much does processing that data at that scale *cost* me?\n",
    "- How can I *optimize* my deployment to minimize the cost incurred at a given scale?\n",
    "\n",
    "In describing how the data below was collected, it will be valuable to establish some more precise language.\n",
    "- A **server** will refer to one instance of the Triton Inference Server deployed on a single GPU-equipped cluster **node**.\n",
    "- A **client** will refer to one instance of a Triton Inference Server client deployed on a single CPU-based GCP **VM** instance. Each client has a unique server to which it sends requests, and each server is associated with the same number of client VMs.\n",
    "- The **scale** of a deployment will refer to the total number of clients and servers, and the resources allocated to each, used in a deployment.\n",
    "- A **frame** will refer to a [gravitational wave frame file](https://www.gw-openscience.org/read_in_c/), a format for encoding multiple **channels** of concurrent timeseries of a fixed length and their associated metadata. The data below was collected by processing 24 frames of length 4096 seconds.\n",
    "- Samples in each frame are mapped onto a fixed time grid at read time according to the desired **sampling rate** (fixed at train time).\n",
    "- In order to parallelize processing across multiple frames, each client is assigned a contiguous (in time) whole-numbered subset of the total. Note that the restriction to whole numbers means that _not all clients will be responsible for the same number of frames_, some will have one more to process than the others. This is a restriction I'm working on relaxing. Each client's subset of data will be referred to as a **stream**.\n",
    "- DeepClean and BBH both expect as input fixed-length **kernels** of data, in this case one second long. Kernels are sampled from streams at a fixed interval termed the **kernel stride**, which is selected at inference time and parameterizes the inference deployment.\n",
    "- In order to minimize network I/O, servers maintain the most recently inferred-upon kernel for each stream as a **state**, which the client updates by sending its stream to the server in kernel stride-length packets. Each of these **requests** triggers a new **inference** for that stream which returns a single BBH event probability estimate as its **response**.\n",
    "- Each server host multiple models which it uses to produce its response.\n",
    "    - `snapshotter` is the model responsible for updating the state and producing a new kernel. It does this for all the witness channels as well as for the strain channels at once.\n",
    "    - `deepclean_h` and `deepclean_l` take a kernel of multiple witness channel measurements and produce an estimate of the noise at either the Hanford or Livingston detectors, respectively.\n",
    "    - `postproc` subtracts the noise estimates from the strain channels.\n",
    "    - `bbh` uses the cleaned strain channels to produce a single scalar estimate of the likelihood that the signature of a binary blackhole merger occurred in the kernel.\n",
    "    - The end-to-end execution of these models is handled by an **ensemble model** called `gwe2e`. The execution of this model is not associated with any particular GPU and so is excluded from the data collected below.\n",
    "- Each model is hosted on every GPU, and a single GPU can even perform concurrent execution of the same model up to a user-defined level of concurrency called the number of model **instances** per GPU.\n",
    "- The number of `snapshotter` instances is equal to the number of streams (since each stream needs its own state to maintain and update).\n",
    "- A model's measured **throughput** is the number of inferences it performed in a given interval on a single GPU, divided by the length of that interval. Its **aggregate throughput** is the sum of that model's throughput across _all_ GPUs. The unit for both of these quantities is **inferences per second**\n",
    "- A model's **queue time** as measured on a given interval is the _average_ amount of time a request had to wait before inference was executed on it during that interval.\n",
    "\n",
    "The data below was collected by requesting inference metrics from each server in a round-robin fashion during the inference run. The collected data is available in CSV format with the following columns\n",
    "- `ip` - the IP address of the server. Used to index different servers.\n",
    "- `step` - indexes the metrics request that a chunk of measurements come from. Also used to roughly align in time metric requests made in serial to different servers.\n",
    "- `gpu_id` - the ID of the GPU executing the model's inference.\n",
    "- `model` - the model that the metrics correspond to.\n",
    "- `process` - subdivides each model's inference into multiple steps. Of particular importance are the `request` process, which measures the end-to-end execution of an inference, and the `queue` process, which measures how long requests spent waiting before execution.\n",
    "- `time (us)` - the _total_ time spent executing the corresponding process over the previous interval in microseconds\n",
    "- `interval` - the time since the last metrics request was made in seconds\n",
    "- `count` - the number of times the corresponding process was executed during the previous interval\n",
    "- `utilization` the current GPU utilization fraction at the time of the metrics request\n",
    "- `average_time` - the average time spent executing the process during the previous interval in microseconds (`time (us)` / `count`)\n",
    "- `throughput` - the rate at which inferences were executed during the interval, measured in inferences per second (`count` / `interval`)\n",
    "- `time_since_start` - the time since the start of the run at which the metrics were sampled.\n",
    "\n",
    "Each CSV is associated with a `RunConfig` object that encodes the unique parameters of the associated inference run, as well as with an 8 character hexadecimal string `RunConfig.id` that identifies the config and is used as the directory where it and its metrics are stored. The configs for which data is available are stored in `plot_utils.configs`, and the metrics collected for a given config can be loaded via `plot_utils.load_stats_for_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1002\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.2.min.js\": \"XypntL49z55iwGVUW4qsEu83zKL3XEcz0MjuGOQ9SlaaQ68X/g+k1FcioZi7oQAc\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.2.min.js\": \"bEsM86IHGDTLCS0Zod8a8WM6Y4+lafAL/eSiyQcuPzinmWNgNO2/olUF0Z2Dkn5i\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.2.min.js\": \"TX0gSQTdXTTeScqxj6PVQxTiRW8DOoGVwinyi1D3kxv7wuxQ02XkOxv0xwiypcAH\"};\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.2.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.2.min.js\": \"XypntL49z55iwGVUW4qsEu83zKL3XEcz0MjuGOQ9SlaaQ68X/g+k1FcioZi7oQAc\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.2.min.js\": \"bEsM86IHGDTLCS0Zod8a8WM6Y4+lafAL/eSiyQcuPzinmWNgNO2/olUF0Z2Dkn5i\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.2.min.js\": \"TX0gSQTdXTTeScqxj6PVQxTiRW8DOoGVwinyi1D3kxv7wuxQ02XkOxv0xwiypcAH\"};\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.2.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RunConfig(num_nodes=2, gpus_per_node=4, clients_per_node=4, instance_config=InstanceConfig(deepclean_h=6, deepclean_l=6, postproc=1, bbh=1), vcpus_per_gpu=16, kernel_stride=0.05, generation_rate=750.0), RunConfig(num_nodes=4, gpus_per_node=4, clients_per_node=4, instance_config=InstanceConfig(deepclean_h=6, deepclean_l=6, postproc=1, bbh=1), vcpus_per_gpu=16, kernel_stride=0.05, generation_rate=750.0), RunConfig(num_nodes=2, gpus_per_node=4, clients_per_node=3, instance_config=InstanceConfig(deepclean_h=6, deepclean_l=6, postproc=1, bbh=1), vcpus_per_gpu=16, kernel_stride=0.1, generation_rate=750.0), RunConfig(num_nodes=2, gpus_per_node=4, clients_per_node=4, instance_config=InstanceConfig(deepclean_h=5, deepclean_l=5, postproc=2, bbh=2), vcpus_per_gpu=16, kernel_stride=0.1, generation_rate=750.0), RunConfig(num_nodes=2, gpus_per_node=4, clients_per_node=5, instance_config=InstanceConfig(deepclean_h=5, deepclean_l=5, postproc=2, bbh=2), vcpus_per_gpu=16, kernel_stride=0.1, generation_rate=750.0), RunConfig(num_nodes=4, gpus_per_node=4, clients_per_node=3, instance_config=InstanceConfig(deepclean_h=6, deepclean_l=6, postproc=1, bbh=1), vcpus_per_gpu=16, kernel_stride=0.1, generation_rate=750.0), RunConfig(num_nodes=4, gpus_per_node=4, clients_per_node=4, instance_config=InstanceConfig(deepclean_h=5, deepclean_l=5, postproc=2, bbh=2), vcpus_per_gpu=16, kernel_stride=0.1, generation_rate=750.0), RunConfig(num_nodes=4, gpus_per_node=4, clients_per_node=4, instance_config=InstanceConfig(deepclean_h=6, deepclean_l=6, postproc=1, bbh=1), vcpus_per_gpu=16, kernel_stride=0.1, generation_rate=750.0), RunConfig(num_nodes=4, gpus_per_node=4, clients_per_node=5, instance_config=InstanceConfig(deepclean_h=6, deepclean_l=6, postproc=1, bbh=1), vcpus_per_gpu=16, kernel_stride=0.1, generation_rate=750.0)]\n"
     ]
    }
   ],
   "source": [
    "import plot_utils\n",
    "\n",
    "print(plot_utils.configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunConfig 26b4090 {\n",
      "\tnum_nodes: 2,\n",
      "\tgpus_per_node: 4,\n",
      "\tclients_per_node: 4,\n",
      "\tinstance_config: {\n",
      "\t\tdeepclean_h: 6,\n",
      "\t\tdeepclean_l: 6,\n",
      "\t\tpostproc: 1,\n",
      "\t\tbbh: 1\n",
      "\t},\n",
      "\tvcpus_per_gpu: 16,\n",
      "\tkernel_stride: 0.05,\n",
      "\tgeneration_rate: 750.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(plot_utils.configs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>step</th>\n",
       "      <th>gpu_id</th>\n",
       "      <th>model</th>\n",
       "      <th>process</th>\n",
       "      <th>time (us)</th>\n",
       "      <th>interval</th>\n",
       "      <th>count</th>\n",
       "      <th>utilization</th>\n",
       "      <th>average_time</th>\n",
       "      <th>throughput</th>\n",
       "      <th>time_since_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.127.76.245</td>\n",
       "      <td>10</td>\n",
       "      <td>5e67acdc-115d-50f1-9bf6-71c2d1e62739</td>\n",
       "      <td>bbh</td>\n",
       "      <td>compute_infer</td>\n",
       "      <td>2.694980e+05</td>\n",
       "      <td>0.215281</td>\n",
       "      <td>122</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2.209000e+03</td>\n",
       "      <td>566.701797</td>\n",
       "      <td>0.215281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34.127.76.245</td>\n",
       "      <td>11</td>\n",
       "      <td>5e67acdc-115d-50f1-9bf6-71c2d1e62739</td>\n",
       "      <td>bbh</td>\n",
       "      <td>compute_infer</td>\n",
       "      <td>2.038580e+05</td>\n",
       "      <td>0.217682</td>\n",
       "      <td>127</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.605181e+03</td>\n",
       "      <td>583.420160</td>\n",
       "      <td>0.432963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.127.76.245</td>\n",
       "      <td>12</td>\n",
       "      <td>5e67acdc-115d-50f1-9bf6-71c2d1e62739</td>\n",
       "      <td>bbh</td>\n",
       "      <td>compute_infer</td>\n",
       "      <td>2.015640e+05</td>\n",
       "      <td>0.217307</td>\n",
       "      <td>122</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.652164e+03</td>\n",
       "      <td>561.417483</td>\n",
       "      <td>0.650270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34.127.76.245</td>\n",
       "      <td>13</td>\n",
       "      <td>5e67acdc-115d-50f1-9bf6-71c2d1e62739</td>\n",
       "      <td>bbh</td>\n",
       "      <td>compute_infer</td>\n",
       "      <td>2.015330e+05</td>\n",
       "      <td>0.212260</td>\n",
       "      <td>111</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.815613e+03</td>\n",
       "      <td>522.942366</td>\n",
       "      <td>0.862530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34.127.76.245</td>\n",
       "      <td>14</td>\n",
       "      <td>5e67acdc-115d-50f1-9bf6-71c2d1e62739</td>\n",
       "      <td>bbh</td>\n",
       "      <td>compute_infer</td>\n",
       "      <td>1.970200e+05</td>\n",
       "      <td>0.209195</td>\n",
       "      <td>118</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.669661e+03</td>\n",
       "      <td>564.067935</td>\n",
       "      <td>1.071725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403695</th>\n",
       "      <td>34.145.51.207</td>\n",
       "      <td>2014</td>\n",
       "      <td>e0dde967-4f8c-e5fa-b37d-a003fabd86d4</td>\n",
       "      <td>snapshotter</td>\n",
       "      <td>request</td>\n",
       "      <td>1.259714e+10</td>\n",
       "      <td>0.230490</td>\n",
       "      <td>120</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.049762e+08</td>\n",
       "      <td>520.628955</td>\n",
       "      <td>458.536346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403696</th>\n",
       "      <td>34.145.51.207</td>\n",
       "      <td>2015</td>\n",
       "      <td>e0dde967-4f8c-e5fa-b37d-a003fabd86d4</td>\n",
       "      <td>snapshotter</td>\n",
       "      <td>request</td>\n",
       "      <td>1.270449e+10</td>\n",
       "      <td>0.215834</td>\n",
       "      <td>121</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.049958e+08</td>\n",
       "      <td>560.616902</td>\n",
       "      <td>458.752180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403697</th>\n",
       "      <td>34.145.51.207</td>\n",
       "      <td>2016</td>\n",
       "      <td>e0dde967-4f8c-e5fa-b37d-a003fabd86d4</td>\n",
       "      <td>snapshotter</td>\n",
       "      <td>request</td>\n",
       "      <td>1.217907e+10</td>\n",
       "      <td>0.221641</td>\n",
       "      <td>116</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.049920e+08</td>\n",
       "      <td>523.368158</td>\n",
       "      <td>458.973821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403698</th>\n",
       "      <td>34.145.51.207</td>\n",
       "      <td>2017</td>\n",
       "      <td>e0dde967-4f8c-e5fa-b37d-a003fabd86d4</td>\n",
       "      <td>snapshotter</td>\n",
       "      <td>request</td>\n",
       "      <td>1.248363e+10</td>\n",
       "      <td>0.218104</td>\n",
       "      <td>119</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.049044e+08</td>\n",
       "      <td>545.610958</td>\n",
       "      <td>459.191925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403699</th>\n",
       "      <td>34.145.51.207</td>\n",
       "      <td>2018</td>\n",
       "      <td>e0dde967-4f8c-e5fa-b37d-a003fabd86d4</td>\n",
       "      <td>snapshotter</td>\n",
       "      <td>request</td>\n",
       "      <td>5.450712e+09</td>\n",
       "      <td>0.215581</td>\n",
       "      <td>52</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.048214e+08</td>\n",
       "      <td>241.208174</td>\n",
       "      <td>459.407507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>403700 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ip  step                                gpu_id  \\\n",
       "0       34.127.76.245    10  5e67acdc-115d-50f1-9bf6-71c2d1e62739   \n",
       "1       34.127.76.245    11  5e67acdc-115d-50f1-9bf6-71c2d1e62739   \n",
       "2       34.127.76.245    12  5e67acdc-115d-50f1-9bf6-71c2d1e62739   \n",
       "3       34.127.76.245    13  5e67acdc-115d-50f1-9bf6-71c2d1e62739   \n",
       "4       34.127.76.245    14  5e67acdc-115d-50f1-9bf6-71c2d1e62739   \n",
       "...               ...   ...                                   ...   \n",
       "403695  34.145.51.207  2014  e0dde967-4f8c-e5fa-b37d-a003fabd86d4   \n",
       "403696  34.145.51.207  2015  e0dde967-4f8c-e5fa-b37d-a003fabd86d4   \n",
       "403697  34.145.51.207  2016  e0dde967-4f8c-e5fa-b37d-a003fabd86d4   \n",
       "403698  34.145.51.207  2017  e0dde967-4f8c-e5fa-b37d-a003fabd86d4   \n",
       "403699  34.145.51.207  2018  e0dde967-4f8c-e5fa-b37d-a003fabd86d4   \n",
       "\n",
       "              model        process     time (us)  interval  count  \\\n",
       "0               bbh  compute_infer  2.694980e+05  0.215281    122   \n",
       "1               bbh  compute_infer  2.038580e+05  0.217682    127   \n",
       "2               bbh  compute_infer  2.015640e+05  0.217307    122   \n",
       "3               bbh  compute_infer  2.015330e+05  0.212260    111   \n",
       "4               bbh  compute_infer  1.970200e+05  0.209195    118   \n",
       "...             ...            ...           ...       ...    ...   \n",
       "403695  snapshotter        request  1.259714e+10  0.230490    120   \n",
       "403696  snapshotter        request  1.270449e+10  0.215834    121   \n",
       "403697  snapshotter        request  1.217907e+10  0.221641    116   \n",
       "403698  snapshotter        request  1.248363e+10  0.218104    119   \n",
       "403699  snapshotter        request  5.450712e+09  0.215581     52   \n",
       "\n",
       "        utilization  average_time  throughput  time_since_start  \n",
       "0              0.99  2.209000e+03  566.701797          0.215281  \n",
       "1              0.99  1.605181e+03  583.420160          0.432963  \n",
       "2              0.99  1.652164e+03  561.417483          0.650270  \n",
       "3              0.38  1.815613e+03  522.942366          0.862530  \n",
       "4              0.38  1.669661e+03  564.067935          1.071725  \n",
       "...             ...           ...         ...               ...  \n",
       "403695         0.39  1.049762e+08  520.628955        458.536346  \n",
       "403696         0.39  1.049958e+08  560.616902        458.752180  \n",
       "403697         0.39  1.049920e+08  523.368158        458.973821  \n",
       "403698         0.39  1.049044e+08  545.610958        459.191925  \n",
       "403699         0.39  1.048214e+08  241.208174        459.407507  \n",
       "\n",
       "[403700 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_utils.load_stats_for_config(plot_utils.configs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can retrieve all the configs matching some desired set of criteria by using `plot_utils.get_configs`, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunConfig 3f14092 {\n",
      "\tnum_nodes: 4,\n",
      "\tgpus_per_node: 4,\n",
      "\tclients_per_node: 4,\n",
      "\tinstance_config: {\n",
      "\t\tdeepclean_h: 6,\n",
      "\t\tdeepclean_l: 6,\n",
      "\t\tpostproc: 1,\n",
      "\t\tbbh: 1\n",
      "\t},\n",
      "\tvcpus_per_gpu: 16,\n",
      "\tkernel_stride: 0.05,\n",
      "\tgeneration_rate: 750.0\n",
      "}\n",
      "RunConfig c58d405d {\n",
      "\tnum_nodes: 4,\n",
      "\tgpus_per_node: 4,\n",
      "\tclients_per_node: 3,\n",
      "\tinstance_config: {\n",
      "\t\tdeepclean_h: 6,\n",
      "\t\tdeepclean_l: 6,\n",
      "\t\tpostproc: 1,\n",
      "\t\tbbh: 1\n",
      "\t},\n",
      "\tvcpus_per_gpu: 16,\n",
      "\tkernel_stride: 0.1,\n",
      "\tgeneration_rate: 750.0\n",
      "}\n",
      "RunConfig c5ec405e {\n",
      "\tnum_nodes: 4,\n",
      "\tgpus_per_node: 4,\n",
      "\tclients_per_node: 4,\n",
      "\tinstance_config: {\n",
      "\t\tdeepclean_h: 5,\n",
      "\t\tdeepclean_l: 5,\n",
      "\t\tpostproc: 2,\n",
      "\t\tbbh: 2\n",
      "\t},\n",
      "\tvcpus_per_gpu: 16,\n",
      "\tkernel_stride: 0.1,\n",
      "\tgeneration_rate: 750.0\n",
      "}\n",
      "RunConfig c626405e {\n",
      "\tnum_nodes: 4,\n",
      "\tgpus_per_node: 4,\n",
      "\tclients_per_node: 4,\n",
      "\tinstance_config: {\n",
      "\t\tdeepclean_h: 6,\n",
      "\t\tdeepclean_l: 6,\n",
      "\t\tpostproc: 1,\n",
      "\t\tbbh: 1\n",
      "\t},\n",
      "\tvcpus_per_gpu: 16,\n",
      "\tkernel_stride: 0.1,\n",
      "\tgeneration_rate: 750.0\n",
      "}\n",
      "RunConfig c6bf405f {\n",
      "\tnum_nodes: 4,\n",
      "\tgpus_per_node: 4,\n",
      "\tclients_per_node: 5,\n",
      "\tinstance_config: {\n",
      "\t\tdeepclean_h: 6,\n",
      "\t\tdeepclean_l: 6,\n",
      "\t\tpostproc: 1,\n",
      "\t\tbbh: 1\n",
      "\t},\n",
      "\tvcpus_per_gpu: 16,\n",
      "\tkernel_stride: 0.1,\n",
      "\tgeneration_rate: 750.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for config in plot_utils.get_configs(num_nodes=4):\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunConfig c6bf405f {\n",
      "\tnum_nodes: 4,\n",
      "\tgpus_per_node: 4,\n",
      "\tclients_per_node: 5,\n",
      "\tinstance_config: {\n",
      "\t\tdeepclean_h: 6,\n",
      "\t\tdeepclean_l: 6,\n",
      "\t\tpostproc: 1,\n",
      "\t\tbbh: 1\n",
      "\t},\n",
      "\tvcpus_per_gpu: 16,\n",
      "\tkernel_stride: 0.1,\n",
      "\tgeneration_rate: 750.0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for config in plot_utils.get_configs(num_nodes=4, clients_per_node=5):\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the first question: how long does it take to process a given amount of data given a certain level of scale? To borrow Erik's terminology, we can look at this in terms of a multipole expansion around the parameter space.\n",
    "\n",
    "At the simplest level, we can search over all the different runs done at a given level of scale, find the configuration that ran in the shortest time, then compare this across levels of scale. We'll also compare across kernel strides, since that will obviously dictate the total number of inferences that we need to do. (Though at time of writing, I've only generated data for kernel strides of 100 ms, so this plot won't be quite as interesting.)\n",
    "\n",
    "I'll note up front that much of this code could be made cleaner and more modular, and I hope to do that sometime this week, but my focus was just on getting these plots together, so I apologize if the code is difficult to make sense of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from bokeh.models import ColumnDataSource, FactorRange\n",
    "from bokeh.io import show\n",
    "\n",
    "times_to_run = defaultdict(lambda : np.inf)\n",
    "color_map = {}\n",
    "color_iter = iter(plot_utils.palette)\n",
    "for config in plot_utils.configs:\n",
    "    df = plot_utils.load_stats_for_config(config)\n",
    "    df = df[(df.model == \"bbh\") & (df.process == \"request\")]\n",
    "    time_to_run = df[\"time_since_start\"].max()\n",
    "\n",
    "    index = (config.num_nodes, config.total_clients, config.kernel_stride)\n",
    "    times_to_run[index] = min(times_to_run[index], time_to_run)\n",
    "    if config.kernel_stride not in color_map:\n",
    "        color_map[config.kernel_stride] = next(color_iter)\n",
    "\n",
    "x = sorted(times_to_run.keys())\n",
    "counts = [times_to_run[key] for key in x]\n",
    "colors = [color_map[key[2]] for key in x]\n",
    "\n",
    "def _make_range(nodes, clients, stride):\n",
    "    return (f\"{nodes} nodes\", f\"{clients} streams\", f\"{stride} s\")\n",
    "x = [_make_range(*key) for key in x]\n",
    "\n",
    "p = plot_utils.make_figure(\n",
    "    title=\"Time to Run vs. Scale\",\n",
    "    x_axis_label=\"Configuration\",\n",
    "    y_axis_label=\"Time to run (s)\",\n",
    "    x_range=FactorRange(*x)\n",
    ")\n",
    "\n",
    "source = ColumnDataSource({\"x\": x, \"counts\": counts, \"colors\": colors})\n",
    "p.vbar(\n",
    "    x=\"x\",\n",
    "    top=\"counts\",\n",
    "    width=0.9,\n",
    "    fill_color=\"colors\",\n",
    "    line_color=\"colors\",\n",
    "    fill_alpha=0.8,\n",
    "    source=source\n",
    ")\n",
    "\n",
    "p.y_range.start = 0\n",
    "p.x_range.range_padding = 0.1\n",
    "p.xaxis.major_label_orientation = 1\n",
    "p.xgrid.grid_line_color = None\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, this might not be the world's most helpful metric if you have a different volume of data that you need to process. So it would perhaps be more insightful to scale by the volume of data as measured in seconds, making the unit along the y-axis *seconds per second of data*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import PrintfTickFormatter\n",
    "\n",
    "seconds_per_seconds = defaultdict(lambda : np.inf)\n",
    "total_time_of_data = 4096 * 24\n",
    "for config in plot_utils.configs:\n",
    "    df = plot_utils.load_stats_for_config(config)\n",
    "    df = df[(df.model == \"bbh\") & (df.process == \"request\")]\n",
    "    time_to_run = df[\"time_since_start\"].max()\n",
    "\n",
    "    index = (config.num_nodes, config.total_clients, config.kernel_stride)\n",
    "    seconds_per_second = time_to_run / total_time_of_data\n",
    "    seconds_per_seconds[index] = min(seconds_per_seconds[index], seconds_per_second)\n",
    "\n",
    "x = sorted(seconds_per_seconds.keys())\n",
    "counts = [seconds_per_seconds[key] for key in x]\n",
    "colors = [color_map[key[2]] for key in x]\n",
    "\n",
    "def _make_range(nodes, clients, stride):\n",
    "    return (f\"{nodes} nodes\", f\"{clients} streams\", f\"{stride} s\")\n",
    "x = [_make_range(*key) for key in x]\n",
    "\n",
    "p = plot_utils.make_figure(\n",
    "    title=\"Time to Run Per Second of Data vs. Scale\",\n",
    "    x_axis_label=\"Configuration\",\n",
    "    y_axis_label=\"Time to run per second of data (s / s')\",\n",
    "    x_range=FactorRange(*x)\n",
    ")\n",
    "p.yaxis[0].formatter = PrintfTickFormatter(format=\"%4.0e\")\n",
    "\n",
    "source = ColumnDataSource({\"x\": x, \"counts\": counts, \"colors\": colors})\n",
    "p.vbar(\n",
    "    x=\"x\",\n",
    "    top=\"counts\",\n",
    "    width=0.9,\n",
    "    fill_color=\"colors\",\n",
    "    line_color=\"black\",\n",
    "    fill_alpha=0.8,\n",
    "    source=source\n",
    ")\n",
    "\n",
    "p.y_range.start = 0\n",
    "p.x_range.range_padding = 0.1\n",
    "p.xaxis.major_label_orientation = 1\n",
    "p.xgrid.grid_line_color = None\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If these point estimates are just the monopole expansion, we can get to the next order by sampling more densely from the distributions which generated these points and plotting the resulting empirical distributions as violin plots. Unfortunately, that would be time and cost intensive to get anywhere near the level of density which would make for a useful plot.\n",
    "\n",
    "However, we can imagine that these total time (and let's call it _time density_) measurements are really the result of summing a bunch of draws from a _throughput_ distribution, a distribution that we _do_ already have lots of samples from.\n",
    "\n",
    "A quick note that Bokeh won't support grouping factors here, so it won't be quite as readable, but still manages to the point across I think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_violin_patch(config, percentile=5, bins=25):\n",
    "    df = plot_utils.load_stats_for_config(config)\n",
    "    df = df[(df.process == \"request\") & (df.model == \"bbh\")]\n",
    "    inferences_per_second = df.groupby(\"step\")[\"throughput\"].agg(\"sum\")\n",
    "    seconds_per_second = 1 / (inferences_per_second * config.kernel_stride)\n",
    "\n",
    "    # TODO: use some sort of KDE instead?\n",
    "    min_, max_ = np.percentile(seconds_per_second, [percentile, 100-percentile])\n",
    "    diff = (max_ - min_) / (bins + 1)\n",
    "    bins = np.linspace(min_ - diff, max_ + diff, bins + 3)\n",
    "    counts, _ = np.histogram(seconds_per_second, bins)\n",
    "    density = counts / (counts.max() * 1.05)\n",
    "    density[0] = density[-1] = 0\n",
    "    bins = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "    xs = list(density / 2) + list(-density[::-1] / 2)\n",
    "    ys = list(bins) + list(bins[::-1])\n",
    "    return xs, ys\n",
    "\n",
    "seconds_per_seconds = defaultdict(lambda : np.inf)\n",
    "best_configs = {}\n",
    "for config in plot_utils.configs:\n",
    "    df = plot_utils.load_stats_for_config(config)\n",
    "    df = df[(df.model == \"bbh\") & (df.process == \"request\")]\n",
    "    time_to_run = df[\"time_since_start\"].max()\n",
    "\n",
    "    index = (config.num_nodes, config.total_clients, config.kernel_stride)\n",
    "    seconds_per_second = time_to_run / total_time_of_data\n",
    "    if seconds_per_second < seconds_per_seconds[index]:\n",
    "        seconds_per_seconds[index] = seconds_per_second\n",
    "        best_configs[index] = config\n",
    "\n",
    "x = sorted(seconds_per_seconds.keys())\n",
    "configs = [best_configs[key] for key in x]\n",
    "colors = [color_map[key[2]] for key in x]\n",
    "\n",
    "def _make_range(nodes, streams, stride):\n",
    "    return f\"{nodes} nodes\\n{streams} streams\\n{stride} stride\"\n",
    "x = [_make_range(*key) for key in x]\n",
    "\n",
    "p = plot_utils.make_figure(\n",
    "    title=\"Time to Run Per Second of Data vs. Scale\",\n",
    "    x_axis_label=\"Configuration\",\n",
    "    y_axis_label=\"Time to run per second of data (s / s')\",\n",
    "    x_range=FactorRange(*x)\n",
    ")\n",
    "p.yaxis[0].formatter = PrintfTickFormatter(format=\"%4.0e\")\n",
    "\n",
    "source = ColumnDataSource()\n",
    "for i, (key, color, config) in enumerate(zip(x, colors, configs)):\n",
    "    x_col, y_col = f\"x_{i}\", f\"y_{i}\"\n",
    "    xs, ys = make_violin_patch(config, percentile=5)\n",
    "    source.add(ys, y_col)\n",
    "\n",
    "    xs = list(zip([key]*len(xs), xs))\n",
    "    source.add(xs, x_col)\n",
    "\n",
    "    p.patch(\n",
    "        x_col,\n",
    "        y_col,\n",
    "        color=color,\n",
    "        alpha=0.6,\n",
    "        line_color=\"black\",\n",
    "        source=source\n",
    "    )\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting from the first question of time to the second one of cost is reasonably straightforward: just input the cost per resource per unit time then multiply by the time density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_per_n1_cpu_per_hour = 0.04749975\n",
    "cpus_per_client = 8\n",
    "cost_per_gpu_per_hour = 0.35\n",
    "\n",
    "def map_to_cost(seconds_per_second, config):\n",
    "    cost_per_gpu = cost_per_gpu_per_hour + cost_per_n1_cpu_per_hour * config.vcpus_per_gpu\n",
    "    server_cost = config.gpus_per_node * cost_per_gpu\n",
    "    return seconds_per_second * server_cost / 3600\n",
    "    \n",
    "\n",
    "cost_per_seconds = defaultdict(lambda : np.inf)\n",
    "for config in plot_utils.configs:\n",
    "    df = plot_utils.load_stats_for_config(config)\n",
    "    df = df[(df.model == \"bbh\") & (df.process == \"request\")]\n",
    "    time_to_run = df[\"time_since_start\"].max()\n",
    "\n",
    "    index = (config.num_nodes, config.total_clients, config.kernel_stride)\n",
    "    seconds_per_second = time_to_run / total_time_of_data\n",
    "    cost_per_second = map_to_cost(seconds_per_second, config)\n",
    "\n",
    "    # this time I'm just going to collect the winning\n",
    "    # configs up front that way I don't have to loop\n",
    "    # through again later\n",
    "    if cost_per_second < cost_per_seconds[index]:\n",
    "        cost_per_seconds[index] = cost_per_second\n",
    "        best_configs[index] = config\n",
    "\n",
    "x = sorted(cost_per_seconds.keys())\n",
    "costs = [cost_per_seconds[key] for key in x]\n",
    "colors = [color_map[key[2]] for key in x]\n",
    "\n",
    "def _make_range(nodes, clients, stride):\n",
    "    return (f\"{nodes} nodes\", f\"{clients} streams\", f\"{stride} s\")\n",
    "\n",
    "factors = [_make_range(*key) for key in x]\n",
    "p = plot_utils.make_figure(\n",
    "    title=\"Cost Per Second of Data vs. Scale\",\n",
    "    x_axis_label=\"Configuration\",\n",
    "    y_axis_label=\"Cost per second of data ($US / s')\",\n",
    "    x_range=FactorRange(*factors)\n",
    ")\n",
    "p.yaxis[0].formatter = PrintfTickFormatter(format=\"%4.0e\")\n",
    "\n",
    "source = ColumnDataSource({\"x\": factors, \"costs\": costs, \"colors\": colors})\n",
    "p.vbar(\n",
    "    x=\"x\",\n",
    "    top=\"costs\",\n",
    "    width=0.9,\n",
    "    fill_color=\"colors\",\n",
    "    line_color=\"black\",\n",
    "    fill_alpha=0.8,\n",
    "    source=source\n",
    ")\n",
    "\n",
    "p.y_range.start = 0\n",
    "p.x_range.range_padding = 0.1\n",
    "p.xaxis.major_label_orientation = 1\n",
    "p.xgrid.grid_line_color = None\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_violin_patch(config, percentile=5, bins=25):\n",
    "    df = plot_utils.load_stats_for_config(config)\n",
    "    df = df[(df.process == \"request\") & (df.model == \"bbh\")]\n",
    "    inferences_per_second = df.groupby(\"step\")[\"throughput\"].agg(\"sum\")\n",
    "    seconds_per_second = 1 / (inferences_per_second * config.kernel_stride)\n",
    "    cost_per_second = map_to_cost(seconds_per_second, config)\n",
    "\n",
    "    # TODO: use some sort of KDE instead?\n",
    "    min_, max_ = np.percentile(cost_per_second, [percentile, 100-percentile])\n",
    "    diff = (max_ - min_) / (bins + 1)\n",
    "    bins = np.linspace(min_ - diff, max_ + diff, bins + 3)\n",
    "    counts, _ = np.histogram(cost_per_second, bins)\n",
    "    density = counts / (counts.max() * 1.05)\n",
    "    density[0] = density[-1] = 0\n",
    "    bins = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "    xs = list(density / 2) + list(-density[::-1] / 2)\n",
    "    ys = list(bins) + list(bins[::-1])\n",
    "    return xs, ys\n",
    "\n",
    "configs = [best_configs[key] for key in x]\n",
    "\n",
    "def _make_range(nodes, streams, stride):\n",
    "    return f\"{nodes} nodes\\n{streams} streams\\n{stride} stride\"\n",
    "\n",
    "p = plot_utils.make_figure(\n",
    "    title=\"Time to Run Per Second of Data vs. Scale\",\n",
    "    x_axis_label=\"Configuration\",\n",
    "    y_axis_label=\"Cost per second of data ($US / s')\",\n",
    "    x_range=FactorRange(*[_make_range(*key) for key in x])\n",
    ")\n",
    "p.yaxis[0].formatter = PrintfTickFormatter(format=\"%4.0e\")\n",
    "\n",
    "source = ColumnDataSource()\n",
    "for i, (key, color, config) in enumerate(zip(x, colors, configs)):\n",
    "    x_col, y_col = f\"x_{i}\", f\"y_{i}\"\n",
    "    xs, ys = make_violin_patch(config, percentile=5)\n",
    "    source.add(ys, y_col)\n",
    "\n",
    "    key = _make_range(*key)\n",
    "    xs = list(zip([key]*len(xs), xs))\n",
    "    source.add(xs, x_col)\n",
    "\n",
    "    p.patch(\n",
    "        x_col,\n",
    "        y_col,\n",
    "        color=color,\n",
    "        alpha=0.6,\n",
    "        line_color=\"black\",\n",
    "        source=source\n",
    "    )\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we might have expected, this plot looks an awful lot like the violin plots for the time density, since each individual violin is just scaled by a constant factor. This might invite the idea of just putting a second y-axis and keeping everything the same, but the issue is that each violin is scaled by a different factor (and as our configurations got more exotic or varied, you can imagine how this might change even more). However, this does leave the possibility of plotting asymmetric violins, with one side plotting the cost distribution and the other side plotting the time distribution. To do this, I'll stop using colors to denote kernel strides and instead use them to color code the sides of the violin to the axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import LinearAxis, Range1d\n",
    "\n",
    "def make_violin_patch(config, percentile=5, bins=25):\n",
    "    df = plot_utils.load_stats_for_config(config)\n",
    "    df = df[(df.process == \"request\") & (df.model == \"bbh\")]\n",
    "    inferences_per_second = df.groupby(\"step\")[\"throughput\"].agg(\"sum\")\n",
    "    seconds_per_second = 1 / (inferences_per_second * config.kernel_stride)\n",
    "    cost_per_second = map_to_cost(seconds_per_second, config)\n",
    "\n",
    "    outputs = []\n",
    "    num_bins = bins\n",
    "    for i, metric in enumerate([seconds_per_second, cost_per_second]):\n",
    "        min_, max_ = np.percentile(metric, [percentile, 100-percentile])\n",
    "        diff = (max_ - min_) / (num_bins + 1)\n",
    "        bins = np.linspace(min_ - diff, max_ + diff, num_bins + 3)\n",
    "        counts, _ = np.histogram(metric, bins)\n",
    "    \n",
    "        sign = (-1)**(i+1)\n",
    "        density = sign * counts / (2 * counts.max() * 1.05)\n",
    "        density[0] = density[-1] = 0\n",
    "        bins = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "        outputs.append([density, bins])\n",
    "    return outputs\n",
    "\n",
    "old_range = p.y_range\n",
    "p = plot_utils.make_figure(\n",
    "    title=\"Time to Run Per Second of Data vs. Scale\",\n",
    "    x_axis_label=\"Configuration\",\n",
    "    y_axis_label=\"Time to run per second of data (s / s')\",\n",
    "    x_range=FactorRange(*[_make_range(*key) for key in x])\n",
    ")\n",
    "p.yaxis[0].formatter = PrintfTickFormatter(format=\"%4.0e\")\n",
    "p.yaxis[0].axis_label_text_color = plot_utils.palette[0]\n",
    "\n",
    "max_cost = max([max(v) for k, v in source.data.items() if k.startswith(\"y\")])\n",
    "p.extra_y_ranges = {\"cost\": Range1d(start=0, end=max_cost)}\n",
    "p.add_layout(\n",
    "    LinearAxis(\n",
    "        y_range_name=\"cost\",\n",
    "        axis_label=\"Cost per second of data ($US / s')\",\n",
    "        axis_label_text_color=plot_utils.palette[1],\n",
    "        formatter=p.yaxis[0].formatter,\n",
    "    ),\n",
    "    \"right\"\n",
    ")\n",
    "\n",
    "source = ColumnDataSource()\n",
    "for i, (key, color, config) in enumerate(zip(x, colors, configs)):\n",
    "    x_time_col, y_time_col = f\"x_time_{i}\", f\"y_time_{i}\"\n",
    "    x_cost_col, y_cost_col = f\"x_cost_{i}\", f\"y_cost_{i}\"\n",
    "\n",
    "    (x_time, y_time), (x_cost, y_cost) = make_violin_patch(config, percentile=5)\n",
    "    source.add(y_time, y_time_col)\n",
    "    source.add(y_cost, y_cost_col)\n",
    "\n",
    "    key = _make_range(*key)\n",
    "    x_time = list(zip([key]*len(x_time), x_time))\n",
    "    x_cost = list(zip([key]*len(x_cost), x_cost))\n",
    "    source.add(x_time, x_time_col)\n",
    "    source.add(x_cost, x_cost_col)\n",
    "\n",
    "    p.patch(\n",
    "        x_time_col,\n",
    "        y_time_col,\n",
    "        color=plot_utils.palette[0],\n",
    "        alpha=0.6,\n",
    "        line_color=\"black\",\n",
    "        source=source\n",
    "    )\n",
    "    p.patch(\n",
    "        x_cost_col,\n",
    "        y_cost_col,\n",
    "        color=plot_utils.palette[1],\n",
    "        alpha=0.6,\n",
    "        line_color=\"black\",\n",
    "        source=source,\n",
    "        y_range_name=\"cost\"\n",
    "    )\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions are still a bit dodgy due to the whole-number enforcement on the frames per client, but I think this graph once appropriately filled out should capture all the 2nd order information of the first two questions.\n",
    "\n",
    "In the next couple of days I will start putting together what graphs I think help answer the optimization question."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
